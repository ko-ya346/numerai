{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b7bfa1c-1bd0-46ba-81ce-637f19d5ca08",
   "metadata": {},
   "source": [
    "# PCA特徴量を追加\n",
    "- 特徴量グループごとに PCA かける\n",
    "- それらを特徴量として保存\n",
    "- (列はいくつ読み込めるのだろう）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f65dde42-b05f-405c-9cab-640105c06958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "391ae95e-faa9-40b0-839f-f86220f50461",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from numerapi import NumerAPI\n",
    "from numerai_tools.scoring import numerai_corr, correlation_contribution\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from src.eda import plot_feature_dist\n",
    "from src.model import MLModel, LightGBMModel\n",
    "from src.validation import caluculate_metrics\n",
    "from src.featuring import feature_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d5a1321-26df-495b-a4ad-b14d71779b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    dataset_path = \"../dataset/v5.0\"\n",
    "    feature_set = \"medium\"\n",
    "    threshold_corr = 0.8\n",
    "    exp = \"eda_20250211\"\n",
    "\n",
    "os.makedirs(cfg.exp, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c738a624-bd82-4f1b-8dcf-1c53a2994d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature col length: 705\n"
     ]
    }
   ],
   "source": [
    "with open(f\"{cfg.dataset_path}/features.json\", \"r\") as f:\n",
    "    feature_metadata = json.load(f)\n",
    "\n",
    "# print(feature_metadata.keys())\n",
    "feature_sets = feature_metadata[\"feature_sets\"]\n",
    "# print(feature_sets.keys())\n",
    "feature_set = feature_sets[cfg.feature_set]\n",
    "print(f\"feature col length: {len(feature_set)}\") \n",
    "targets = [\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a61b9415-0895-44e6-91d0-1ae4edf2ce31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2461\n",
      "2371\n"
     ]
    }
   ],
   "source": [
    "feature_groups = [\n",
    "    \"intelligence\",\n",
    "    \"charisma\",\n",
    "    \"strength\",\n",
    "    \"dexterity\",\n",
    "    \"constitution\",\n",
    "    \"wisdom\",\n",
    "    \"agility\",\n",
    "    \"serenity\",\n",
    "    \"sunshine\",\n",
    "    \"rain\",\n",
    "    \"midnight\",\n",
    "]\n",
    "cnt_col = 0\n",
    "col_set = set([])\n",
    "for group in feature_groups:\n",
    "    cnt_col += len(feature_sets[group])\n",
    "    col_set |= set(feature_sets[group])\n",
    "print(cnt_col)\n",
    "print(len(list(col_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ede6aa2d-f81e-4047-b888-6f967d18a2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "intelligence\n",
      "68\n",
      "28\n",
      "================================================================\n",
      "charisma\n",
      "600\n",
      "233\n",
      "================================================================\n",
      "strength\n",
      "302\n",
      "109\n",
      "================================================================\n",
      "dexterity\n",
      "120\n",
      "42\n",
      "================================================================\n",
      "constitution\n",
      "754\n",
      "243\n",
      "================================================================\n",
      "wisdom\n",
      "367\n",
      "115\n",
      "================================================================\n",
      "agility\n",
      "480\n",
      "121\n",
      "================================================================\n",
      "serenity\n",
      "137\n",
      "64\n",
      "================================================================\n",
      "sunshine\n",
      "542\n",
      "232\n",
      "================================================================\n",
      "rain\n",
      "21\n",
      "19\n",
      "================================================================\n",
      "midnight\n",
      "332\n",
      "142\n"
     ]
    }
   ],
   "source": [
    "drop_cols_d = {}\n",
    "\n",
    "for feature_group_name in feature_groups:\n",
    "    print(\"=\" * 64)\n",
    "    print(feature_group_name)\n",
    "    feature_set = feature_sets[feature_group_name]\n",
    "    feature_set = feature_set[:300]\n",
    "    \n",
    "    train = pd.read_parquet(\n",
    "        f\"{cfg.dataset_path}/train.parquet\",\n",
    "        columns=[\"era\"] + feature_set + targets\n",
    "    )\n",
    "    \n",
    "    valid = pd.read_parquet(\n",
    "        f\"{cfg.dataset_path}/validation.parquet\",\n",
    "        columns=[\"era\"] + feature_set + targets\n",
    "        \n",
    "    )\n",
    "    \n",
    "    data = pd.concat([train, valid])\n",
    "    \n",
    "    \n",
    "    correlation_matrix = data[feature_set].corr()\n",
    "    cols = sorted(correlation_matrix.columns)\n",
    "    \n",
    "    high_corr_pairs = [\n",
    "        (cols[i], cols[j]) \n",
    "        for i in range(len(cols))\n",
    "        for j in range(i + 1, len(cols))\n",
    "        if abs(correlation_matrix.loc[cols[i], cols[j]]) > cfg.threshold_corr\n",
    "    ]\n",
    "    drop_cols = list(set([cols[0] for cols in high_corr_pairs]))\n",
    "    \n",
    "    # print(high_corr_pairs)\n",
    "    # print(drop_cols)\n",
    "    print(len(high_corr_pairs))\n",
    "    print(len(drop_cols))\n",
    "    \n",
    "    drop_cols_d[feature_group_name] = drop_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8513923-bf5b-4e14-bc44-b587a8e5f1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(cfg.exp, \"feature_groups_drop_cols.json\")\n",
    "with open(filepath, \"w\") as f:\n",
    "    json.dump(drop_cols_d, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b37fb1-1335-4b48-8676-15994d76afc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3ab7498-0c52-4413-bd8a-4acbc2e08c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "intelligence\n",
      "28\n",
      "charisma\n",
      "233\n",
      "strength\n",
      "109\n",
      "dexterity\n",
      "42\n",
      "constitution\n",
      "243\n",
      "wisdom\n",
      "115\n",
      "agility\n",
      "121\n",
      "serenity\n",
      "64\n",
      "sunshine\n",
      "232\n",
      "rain\n",
      "19\n",
      "midnight\n",
      "142\n",
      "total:  1348\n"
     ]
    }
   ],
   "source": [
    "with open(filepath, \"r\") as f:\n",
    "    drop_cols_d = json.load(f)\n",
    "\n",
    "print(len(drop_cols_d.keys()))\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "\n",
    "for k, v in drop_cols_d.items():\n",
    "    print(k)\n",
    "    print(len(v))\n",
    "    cnt += len(v)\n",
    "print(\"total: \", cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42ebc712-55c6-41c2-a294-f4e9a125de20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6324097, 11)\n"
     ]
    }
   ],
   "source": [
    "emb_df = feature_pca(data[feature_set], n_components=len(feature_set) // 3)\n",
    "print(emb_df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
